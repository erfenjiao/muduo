# 01 大纲
## 知识储备

1. TCP/UDP 协议
2. TCP/UDP 编程步骤
3. IO 复用接口编程 select poll epoll
4. Linux 的多线程编程pthread 进程和线程模型

### 五种 IO 模型

1. 阻塞 blocking
2. 非阻塞 blocking
3. IO 复用
4. 信号驱动
5. 异步

## 目的
   1. 理解阻塞、非阻塞、同步、异步
   2. 理解五种 IO 模型
   3. epoll
   4. Reactor
   5. 网络层：muduo libevent 缓存层：redis memerycache 存储层：mysql mongoDB
   6. 掌握基于事件驱动和事件回调的epoll+线程池面向对象编程

# 02 阻塞、非阻塞、同步、异步
我：
阻塞：执行一个事件，主线程暂停，直到事件完成后返回，继续执行主线程

非阻塞：事件的触发并不影响主线程的继续执行

——————

典型的一次 IO 的两个阶段是什么？ 数据准备和数据读写。

数据准备:阻塞与非阻塞

阻塞：调用IO方法的线程进入阻塞状态

非阻塞：不会改变线程的状态，通过返回值判断

```c
#include <sys/types.h>
#include <sys/socket.h>

ssize_t recv(int sockfd, void *buf, size_t len, int flags);

int size = recv(sockfd, buf, 1024, 0);
// sockfd 默认阻塞
// 若是将 sockfd 设置为非阻塞，recv 会直接返回
// 如果 sockfd 没有数据准备好的话，会不断的空转 CPU

// size == -1                    
// size == 0 && errno = EAGAIN  非阻塞 IO 返回
// size > 0                     读到数据
```

数据读写：IO的同步与异步（强调IO， 因为还有一种并发的同步和异步，是业务上的）

同步
```c
#include <sys/types.h>
#include <sys/socket.h>

ssize_t recv(int sockfd, void *buf, size_t len, int flags);

// 同步
char buf[1024] = {0};
int size = recv(sockfd, buf, 1024, 0);
// 如果 buf 中有了数据， recv 接收数据，应用程序读，直到数据搬完为止
```

异步

异步IO模型是比较理想的IO模型，在异步IO模型中，当用户线程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从内核的角度，当它受到一个asynchronous read之后，它会立刻返回，说明read请求已经成功发起了，因此不会对用户线程产生任何block。然后，内核会等待数据准备完成，然后将数据拷贝到用户线程，当这一切都完成之后，内核会给用户线程发送一个信号，告诉它read操作完成了。也就说用户线程完全不需要关心实际的整个IO操作是如何进行的，只需要先发起一个请求，当接收内核返回的成功信号时表示IO操作已经完成，可以直接去使用数据了。

也就说在异步IO模型中，IO操作的两个阶段都不会阻塞用户线程，这两个阶段都是由内核自动完成，然后发送一个信号告知用户线程操作已完成。用户线程中不需要再次调用IO函数进行具体的读写。

重点：通知信号

![Alt text](图片/1.png)

异步IO接口：aio_read aio_write

详细解读：https://www.cnblogs.com/schips/p/12575933.html

```c
#include <aio.h>

int aio_read(struct aiocb *__aiocbp);
int aio_write(struct aiocb *__aiocbp);

Link with -lrt.
    
/* 有关结构体 ，能够使用的成员已经标出 */
struct aiocb
{
▲ int aio_fildes;       	/* 对哪个文件进行读写.  */
▲ int aio_lio_opcode;       /* 要执行的操作  */
  int aio_reqprio;      	/* Request priority offset.  */
▲ volatile void *aio_buf;   /* 读写用的buffer  */
▲ size_t aio_nbytes;        /* Length of transfer.  */
▲ struct sigevent aio_sigevent; /* 告诉 AIO 在 I/O 操作完成时应该执行什么操作。  */

  /* Internal members.  */
  struct aiocb *__next_prio;
  int __abs_prio;
  int __policy;
  int __error_code;
  __ssize_t __return_value;

#ifndef __USE_FILE_OFFSET64	// 针对大文件的支持
▲ __off_t aio_offset;       /* 在传统的 read 调用中，偏移量是在文件描述符上下文中进行维护的，  */
  char __pad[sizeof (__off64_t) - sizeof (__off_t)];
#else
▲ __off64_t aio_offset;     /* 对于异步 I/O 操作来说这是不可能的，因为我们可以同时执行很多读请求，因此必须为每个特定的读请求都指定偏移量。 */
#endif
  char __glibc_reserved[32];
};


struct sigevent {
    int sigev_notify; /* 通知方式：为SIGEV_NONE、SIGEV_SIGNAL、SIGEV_THREAD、SIGEV_THREAD_ID（只针对linux）当中的一个； */
    int sigev_signo;  /* 为signal的值，当sigev_notify为SIGEV_SIGNAL时，会将这个signal发送给进程； */
    union sigval sigev_value;  /* 信号传递的数据 */
    void (*sigev_notify_function) (union sigval);/* 当sigev_notify为SIGEV_THREAD时，处理线程将调用这个处理函数 (SIGEV_THREAD) */
    void *sigev_notify_attributes;/* sigev_notify_function的参数 (SIGEV_THREAD) */
    pid_t sigev_notify_thread_id; /* 当sigev_notify为SIGEV_THREAD_ID时的处理线程ID (SIGEV_THREAD_ID) */
};

union sigval {               /*传递的参数*/
    int     sival_int;        /* 信号机制传递的参数 */
    void   *sival_ptr;        /* 若是线程机制传递的参数 */
};

//  什么时候使用 AIO ？了解 AIO 机制之后，不难发现， AIO 其实是用于解决大量 IO 并发操作而出现的，牺牲一些信号处理耗时，用多线程的方式加速 IO ，提高并行效率。 
```

# 07 良好的服务器应该怎样设计？
## one loop per thread is usually a good model

这样多线程服务端编程的问题就转化为如何设计一个高效易用的 event loop， 然后每个线程run一个 event loop即可。

event loop 是 non-blocking， 在现实生活中， non-blocking 一般和 IO-multiplexing 一起使用，原因有两点：

1. 没有人真的会用轮询(busy-pooling)来检查某个non-blocking IO 操作是否完成，这样太浪费CPU资源了
2. IO-multiplex 一般不能和 blocking IO 用在一起，因为blocking IO 中 read()/write()/accept()/connect() 都有可能阻塞当前线程，这样线程就没有办法处理其他 socket 上的 IO 事件了。 

nginx 服务器采用了 epoll+fork 模型作为网络模块的架构设计，实现了简单好用的负载算法，使各个 fork 网络进程不会忙的越忙、闲的越闲，并且通过引入一把乐观锁解决了该模型导致的服务器惊群现象，功能十分强大。

# 08 Reactor 模型
重要组件：Event事件、Reactor反应堆、Demultiplex事件分发器、Evanthandler事件处理器

> The reactor design pattern is an event handing for handing service requests deliverd concurrently to a service handler by one or more inputs.
> The service handler then demultiplexes the incoming requests and dispatches them syncronously to the associated request handlers

